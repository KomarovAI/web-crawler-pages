name: Scrape Website
on:
  workflow_dispatch:
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Scrape callmedley.com
        run: |
          mkdir -p sites/callmedley_com/pages
          wget -r -l 3 -k -p --no-parent --timeout=10 -q \
            -P sites/callmedley_com/ \
            https://callmedley.com/ 2>&1 || echo "wget completed with exit code $?"
          find sites/callmedley_com -type f -name '*.html' | sort > /tmp/all_files.txt
          echo "Total files: $(wc -l < /tmp/all_files.txt)"
      - name: Process pages
        run: |
          python3 << 'PYTHON_EOF'
import os
import shutil
from pathlib import Path

all_files = []
with open('/tmp/all_files.txt', 'r') as f:
    all_files = [line.strip() for line in f.readlines() if line.strip()]

if not all_files:
    print("No files found")
    exit(0)

print(f"Processing {len(all_files)} files")

pages_dir = Path('sites/callmedley_com/pages')
pages_dir.mkdir(parents=True, exist_ok=True)

page_mapping = []
for idx, file_path in enumerate(all_files[:200], 1):
    target_name = f'page-{idx}.html'
    target_path = pages_dir / target_name
    try:
        shutil.copy2(file_path, target_path)
        url_like = 'https://callmedley.com/' + file_path.replace('sites/callmedley_com/', '').replace('index.html', '').rstrip('/')
        page_mapping.append((target_name, url_like))
    except:
        pass

html_lines = [
    '<!DOCTYPE html>',
    '<html lang="en">',
    '<head>',
    '<meta charset="UTF-8">',
    '<meta name="viewport" content="width=device-width, initial-scale=1.0">',
    '<title>callmedley_com</title>',
    '<style>',
    '* { margin: 0; padding: 0; }',
    'body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; background: #f5f5f5; padding: 20px; }',
    '.container { max-width: 1000px; margin: 0 auto; }',
    'h1 { color: #333; margin-bottom: 30px; }',
    '.page-list { list-style: none; }',
    '.page-item { background: white; padding: 15px; margin-bottom: 10px; border-radius: 6px; border-left: 4px solid #667eea; }',
    'a { color: #667eea; text-decoration: none; }',
    'a:hover { text-decoration: underline; }',
    '</style>',
    '</head>',
    '<body>',
    '<div class="container">',
    f'<h1>üìÑ callmedley_com ({len(page_mapping)} pages)</h1>',
    '<ul class="page-list">'
]

for page_file, url in page_mapping:
    html_lines.append(f'<li class="page-item"><a href="{page_file}">üîó {url}</a></li>')

html_lines.extend([
    '</ul>',
    '</div>',
    '</body>',
    '</html>'
])

index_path = Path('sites/callmedley_com/index.html')
with open(index_path, 'w', encoding='utf-8') as f:
    f.write('\n'.join(html_lines))

print(f"Generated index with {len(page_mapping)} pages")
PYTHON_EOF
      - name: Commit scraped content
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add sites/callmedley_com/ || true
          git commit -m "üï∑Ô∏è Scraped website" || true
          git push origin main || echo "Nothing to push"
